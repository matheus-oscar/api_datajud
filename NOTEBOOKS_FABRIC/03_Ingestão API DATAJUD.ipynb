{"cells":[{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","from datetime import datetime\n","\n","import ast\n","import json\n","from datetime import datetime\n","\n","from pyspark.sql.functions import lit\n","from pyspark.sql import DataFrame\n","from pyspark.sql.functions import col, from_json, array_join, transform, explode_outer, to_timestamp, from_unixtime, lit\n","from pyspark.sql.types import ArrayType, StructType, StringType, LongType"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"019b0dda-afc2-4074-9241-4fe0e9137764","normalized_state":"finished","queued_time":"2025-08-21T13:53:14.9514535Z","session_start_time":null,"execution_start_time":"2025-08-21T13:53:21.8878166Z","execution_finish_time":"2025-08-21T13:53:31.2687916Z","parent_msg_id":"5a5bb385-820c-4d54-b421-c23b2feb19ad"},"text/plain":"StatementMeta(, 019b0dda-afc2-4074-9241-4fe0e9137764, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c73b0cd3-9b4c-4446-9957-851b7d13cc80"},{"cell_type":"code","source":["def expandir_colunas(df, colunas_alvo=None, chaves=[\"codigo\", \"nome\"], separador=\"|\", ordem_final=None):\n","    \"\"\"\n","    Expande colunas que são listas de dicionários em colunas separadas com os campos desejados (ex: codigo, nome).\n","    Suporta colunas como string JSON ou já como StructType/ArrayType(StructType).\n","\n","    Parâmetros:\n","        df: DataFrame Spark original\n","        colunas_alvo: lista de colunas a expandir (default = ['classe', 'sistema', 'formato', 'orgaoJulgador', 'assuntos'])\n","        chaves: chaves dentro de cada dict a extrair (ex: 'codigo', 'nome')\n","        separador: separador entre elementos concatenados\n","        ordem_final: ordem final das colunas no DataFrame (lista)\n","\n","    Retorna:\n","        DataFrame com colunas expandidas e reorganizadas.\n","    \"\"\"\n","    if colunas_alvo is None:\n","        colunas_alvo = ['classe', 'sistema', 'formato', 'orgaoJulgador', 'assuntos']\n","\n","    for coluna in colunas_alvo:\n","        tipo_coluna = df.schema[coluna].dataType\n","\n","        # Se for string, assume que é uma lista de dicts em string JSON\n","        if isinstance(tipo_coluna, StringType):\n","            schema_array = ArrayType(StructType().add(\"codigo\", StringType()).add(\"nome\", StringType()))\n","            df = df.withColumn(f\"__parsed_{coluna}\", from_json(col(coluna), schema_array))\n","            for chave in chaves:\n","                nova_coluna = f\"{coluna}_{chave}\"\n","                df = df.withColumn(nova_coluna, array_join(transform(col(f\"__parsed_{coluna}\"), lambda x: x[chave]), separador))\n","\n","        # Se já for ArrayType(StructType(...))\n","        elif isinstance(tipo_coluna, ArrayType):\n","            if isinstance(tipo_coluna.elementType, StructType):\n","                for chave in chaves:\n","                    nova_coluna = f\"{coluna}_{chave}\"\n","                    df = df.withColumn(nova_coluna, array_join(transform(col(coluna), lambda x: x[chave]), separador))\n","\n","        # Se for StructType\n","        elif isinstance(tipo_coluna, StructType):\n","            for chave in chaves:\n","                nova_coluna = f\"{coluna}_{chave}\"\n","                df = df.withColumn(nova_coluna, col(f\"{coluna}.{chave}\"))\n","\n","        else:\n","            raise ValueError(f\"Tipo não esperado para coluna {coluna}: {tipo_coluna}\")\n","\n","    # Reordena as colunas, se desejado\n","    if ordem_final:\n","        colunas_disponiveis = [c for c in ordem_final if c in df.columns]\n","        df = df.select(*colunas_disponiveis)\n","\n","    return df\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"019b0dda-afc2-4074-9241-4fe0e9137764","normalized_state":"finished","queued_time":"2025-08-21T13:53:40.5834178Z","session_start_time":null,"execution_start_time":"2025-08-21T13:53:40.5846094Z","execution_finish_time":"2025-08-21T13:53:40.928044Z","parent_msg_id":"cecaaca5-a217-40f6-a52b-11509f3c2f65"},"text/plain":"StatementMeta(, 019b0dda-afc2-4074-9241-4fe0e9137764, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"909c46a3-1373-4d11-b34a-f42ff2286759"},{"cell_type":"code","source":["CAMINHO_PASTA_LOTES = \"Files/DATAJUD/resultado-api/lotes\"\n","TABELA_API = \"api_datajud\"\n","TABELA_MOVIMENTOS = \"movimentos_datajud\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"019b0dda-afc2-4074-9241-4fe0e9137764","normalized_state":"finished","queued_time":"2025-08-21T13:53:44.56508Z","session_start_time":null,"execution_start_time":"2025-08-21T13:53:44.566264Z","execution_finish_time":"2025-08-21T13:53:44.9346116Z","parent_msg_id":"3e06a82e-a7f6-4ce6-9e11-59ffe1df41d4"},"text/plain":"StatementMeta(, 019b0dda-afc2-4074-9241-4fe0e9137764, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"048b0c8b-a69d-4e44-9ce3-663808cf4040"},{"cell_type":"code","source":["# Leitura de todos os arquivos Parquet na pasta\n","df_lotes = spark.read.format(\"parquet\").load(CAMINHO_PASTA_LOTES)\n","\n","print(f\"Registros carregados dos lotes: {df_lotes.count()}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"019b0dda-afc2-4074-9241-4fe0e9137764","normalized_state":"finished","queued_time":"2025-08-21T13:53:48.4367334Z","session_start_time":null,"execution_start_time":"2025-08-21T13:53:48.4378905Z","execution_finish_time":"2025-08-21T14:00:42.9556583Z","parent_msg_id":"8fa7c535-e8b5-47fe-911d-c94ae8fb2a37"},"text/plain":"StatementMeta(, 019b0dda-afc2-4074-9241-4fe0e9137764, 6, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Registros carregados dos lotes: 20743476\n"]}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8e759925-3635-4bbf-928a-e04bb8f98587"},{"cell_type":"code","source":["# Verifica se a tabela Delta já existe\n","tabelas_existentes = [t.name for t in spark.catalog.listTables()]\n","tabela_existe = TABELA_API in tabelas_existentes\n","\n","ordem_colunas = [\n","    'numeroProcesso', 'classe', 'classe_codigo', 'classe_nome',  \n","    'sistema', 'sistema_codigo', 'sistema_nome', 'formato', 'formato_codigo',\n","    'formato_nome', 'tribunal', 'dataHoraUltimaAtualizacao', 'grau',\n","    '@timestamp', 'dataAjuizamento', 'movimentos', 'id', 'nivelSigilo',\n","    'orgaoJulgador', 'orgaoJulgador_codigo', 'orgaoJulgador_nome',\n","    'assuntos', 'assuntos_codigo', 'assuntos_nome', '_id', 'data_download'\n","]\n","\n","df_expandido = expandir_colunas(df_lotes, ordem_final=ordem_colunas)\n","\n","# Adiciona coluna de data de ingestão \n","df_expandido = df_expandido.withColumn(\"data_ingestao\", lit(datetime.now().isoformat()))\n","\n","if not tabela_existe:\n","    print(f\"Criando nova tabela Delta: {TABELA_API}\")\n","    df_expandido.write.format(\"delta\").mode(\"overwrite\").saveAsTable(TABELA_API)\n","else:\n","    print(f\"Tabela já existe. Iniciando merge com deduplicação por _id\")\n","\n","    # Lê tabela existente\n","    df_existente = spark.table(TABELA_API).select(\"_id\")\n","\n","    # Remove duplicados (merge incremental)\n","    df_novos = df_expandido.join(df_existente, on=\"_id\", how=\"left_anti\")\n","\n","    if df_novos.count() == 0:\n","        print(\"✅ Nenhum dado novo a inserir.\")\n","    else:\n","        print(f\"Inserindo {df_novos.count()} novos registros...\")\n","        df_novos.write.format(\"delta\").mode(\"append\").saveAsTable(TABELA_API)\n","\n","print(\"✅ Ingestão finalizada.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"15a409ad-16d3-4cb0-b346-0596a128e0ca","normalized_state":"finished","queued_time":"2025-08-19T21:01:36.4762747Z","session_start_time":null,"execution_start_time":"2025-08-19T21:01:36.4777833Z","execution_finish_time":"2025-08-19T21:07:08.4341392Z","parent_msg_id":"0a4955df-cb1c-4c40-9d8e-6304a756b846"},"text/plain":"StatementMeta(, 15a409ad-16d3-4cb0-b346-0596a128e0ca, 7, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Criando nova tabela Delta: api_datajud\n✅ Ingestão finalizada.\n"]}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"advisor":{"adviceMetadata":"{\"artifactId\":\"85b8f3d7-acef-4c77-9177-790b900f0304\",\"activityId\":\"15a409ad-16d3-4cb0-b346-0596a128e0ca\",\"applicationId\":\"application_1755636609720_0001\",\"jobGroupId\":\"7\",\"advices\":{\"info\":1,\"warn\":1}}"}},"id":"cdc2e6db-4027-4278-b1b0-5067b1483245"},{"cell_type":"markdown","source":["### TABELA DE MOVIMENTAÇÕES"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"234456d6-fc9e-4e69-bcb9-403196ecdfd0"},{"cell_type":"code","source":["# from pyspark.sql import DataFrame\n","# from pyspark.sql.functions import col, explode_outer, to_timestamp, from_unixtime, lit\n","# from pyspark.sql.types import LongType\n","\n","def extrair_movimentos_completos(df: DataFrame) -> DataFrame:\n","    \"\"\"\n","    Extrai os movimentos e seus complementos de um DataFrame Spark que contém uma coluna 'movimentos',\n","    estruturada como lista de structs. Retorna um DataFrame ordenado por _id e dataHora.\n","    \"\"\"\n","\n","    df_mov = df.withColumn(\"movimento\", explode_outer(\"movimentos\"))\n","\n","    df_mov = df_mov.withColumn(\"complemento\", explode_outer(\"movimento.complementosTabelados\"))\n","\n","    df_resultado = df_mov.select(\n","        col(\"_id\"),\n","        col(\"numeroProcesso\"),\n","        col(\"movimento.codigo\").alias(\"movimentos_codigo\"),\n","        col(\"movimento.nome\").alias(\"movimentos_nome\"),\n","        to_timestamp(col(\"movimento.dataHora\")).alias(\"movimentos_dataHora\"),\n","        col(\"complemento.codigo\").alias(\"movimentos_complementosTabelados_codigo\"),\n","        col(\"complemento.descricao\").alias(\"movimentos_complementosTabelados_descricao\"),\n","        col(\"complemento.valor\").alias(\"movimentos_complementosTabelados_valor\"),\n","        col(\"complemento.nome\").alias(\"movimentos_complementosTabelados_nome\"),\n","        col(\"data_download\")\n","        \n","    )\n","\n","    return df_resultado.orderBy([\"_id\", \"movimentos_dataHora\"], ascending=[True, False])"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"019b0dda-afc2-4074-9241-4fe0e9137764","normalized_state":"finished","queued_time":"2025-08-21T14:14:08.3215793Z","session_start_time":null,"execution_start_time":"2025-08-21T14:14:08.3226417Z","execution_finish_time":"2025-08-21T14:14:08.7672113Z","parent_msg_id":"8cfec6d8-a8fb-4dbe-b6d6-4dccb24bd688"},"text/plain":"StatementMeta(, 019b0dda-afc2-4074-9241-4fe0e9137764, 7, Finished, Available, Finished)"},"metadata":{}}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"066832ce-ce2c-4d45-bb87-18c799e02dc1"},{"cell_type":"code","source":["movimentos = spark.sql(\"\"\"SELECT _id, numeroProcesso, movimentos, data_download\n","FROM DOL_arqs_auxiliares.api_datajud\"\"\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"019b0dda-afc2-4074-9241-4fe0e9137764","normalized_state":"finished","queued_time":"2025-08-21T14:14:11.4586349Z","session_start_time":null,"execution_start_time":"2025-08-21T14:14:11.4597663Z","execution_finish_time":"2025-08-21T14:15:10.1633795Z","parent_msg_id":"3eaad02a-a8de-46c6-ae58-129d0d568d08"},"text/plain":"StatementMeta(, 019b0dda-afc2-4074-9241-4fe0e9137764, 8, Finished, Available, Finished)"},"metadata":{}}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"037795da-5362-4129-a638-9b04010a9011"},{"cell_type":"code","source":["mov_datajud = extrair_movimentos_completos(movimentos)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"019b0dda-afc2-4074-9241-4fe0e9137764","normalized_state":"finished","queued_time":"2025-08-21T14:15:14.7990569Z","session_start_time":null,"execution_start_time":"2025-08-21T14:15:14.8002141Z","execution_finish_time":"2025-08-21T14:15:15.0950542Z","parent_msg_id":"8dd59251-e95d-4df7-9904-399e25db882d"},"text/plain":"StatementMeta(, 019b0dda-afc2-4074-9241-4fe0e9137764, 9, Finished, Available, Finished)"},"metadata":{}}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"9c9c5548-404c-4fd5-a575-35e7b81f1e85"},{"cell_type":"code","source":["display(mov_datajud.tail(100))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"019b0dda-afc2-4074-9241-4fe0e9137764","normalized_state":"finished","queued_time":"2025-08-21T14:38:53.5979553Z","session_start_time":null,"execution_start_time":"2025-08-21T14:38:53.5992032Z","execution_finish_time":"2025-08-21T14:42:31.066444Z","parent_msg_id":"17a3bcf8-abab-4783-9192-905f31d5623d"},"text/plain":"StatementMeta(, 019b0dda-afc2-4074-9241-4fe0e9137764, 12, Finished, Available, Finished)"},"metadata":{}},{"output_type":"error","ename":"Py4JJavaError","evalue":"An error occurred while calling o7896.tailToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 25 in stage 38.0 failed 4 times, most recent failure: Lost task 25.3 in stage 38.0 (TID 7458) (vm-4d492599 executor 43): ExecutorLostFailure (executor 43 exited caused by one of the running tasks) Reason: Container from a bad node: container_1755784154804_0001_01_000044 on host: vm-4d492599. Exit status: 137. Diagnostics: [2025-08-21 14:42:29.059]Container killed on request. Exit code is 137\n[2025-08-21 14:42:29.081]Container exited with a non-zero exit code 137. \n[2025-08-21 14:42:29.084]Killed by external signal\n.\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3055)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2991)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2990)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2990)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1294)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1294)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1294)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3262)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3193)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3182)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1028)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2568)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2589)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2608)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2633)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1056)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:411)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1055)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:320)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:187)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:296)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:179)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:173)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:149)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:145)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$submitShuffleJob$1(ShuffleExchangeExec.scala:73)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:268)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:265)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob(ShuffleExchangeExec.scala:73)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob$(ShuffleExchangeExec.scala:72)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.submitShuffleJob(ShuffleExchangeExec.scala:120)\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture$lzycompute(QueryStageExec.scala:188)\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture(QueryStageExec.scala:188)\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:190)\n\tat org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:61)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$6(AdaptiveSparkPlanExec.scala:297)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$6$adapted(AdaptiveSparkPlanExec.scala:295)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:295)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:961)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:266)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:412)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeTail(AdaptiveSparkPlanExec.scala:393)\n\tat org.apache.spark.sql.Dataset.$anonfun$tailToPython$1(Dataset.scala:4184)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4348)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:810)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4346)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:132)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:220)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:961)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4346)\n\tat org.apache.spark.sql.Dataset.tailToPython(Dataset.scala:4181)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m display(mov_datajud\u001b[38;5;241m.\u001b[39mtail(\u001b[38;5;241m100\u001b[39m))\n","File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:1439\u001b[0m, in \u001b[0;36mDataFrame.tail\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m \u001b[38;5;124;03mReturns the last ``num`` rows as a :class:`list` of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1436\u001b[0m \u001b[38;5;124;03m[Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1437\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1439\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mtailToPython(num)\n\u001b[1;32m   1440\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n","File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n","File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o7896.tailToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 25 in stage 38.0 failed 4 times, most recent failure: Lost task 25.3 in stage 38.0 (TID 7458) (vm-4d492599 executor 43): ExecutorLostFailure (executor 43 exited caused by one of the running tasks) Reason: Container from a bad node: container_1755784154804_0001_01_000044 on host: vm-4d492599. Exit status: 137. Diagnostics: [2025-08-21 14:42:29.059]Container killed on request. Exit code is 137\n[2025-08-21 14:42:29.081]Container exited with a non-zero exit code 137. \n[2025-08-21 14:42:29.084]Killed by external signal\n.\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3055)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2991)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2990)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2990)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1294)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1294)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1294)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3262)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3193)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3182)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1028)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2568)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2589)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2608)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2633)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1056)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:411)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1055)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:320)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:187)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:296)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:179)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:173)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:149)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:145)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$submitShuffleJob$1(ShuffleExchangeExec.scala:73)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:268)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:265)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob(ShuffleExchangeExec.scala:73)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob$(ShuffleExchangeExec.scala:72)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.submitShuffleJob(ShuffleExchangeExec.scala:120)\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture$lzycompute(QueryStageExec.scala:188)\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture(QueryStageExec.scala:188)\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:190)\n\tat org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:61)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$6(AdaptiveSparkPlanExec.scala:297)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$6$adapted(AdaptiveSparkPlanExec.scala:295)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:295)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:961)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:266)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:412)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeTail(AdaptiveSparkPlanExec.scala:393)\n\tat org.apache.spark.sql.Dataset.$anonfun$tailToPython$1(Dataset.scala:4184)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4348)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:810)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4346)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:132)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:220)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:961)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4346)\n\tat org.apache.spark.sql.Dataset.tailToPython(Dataset.scala:4181)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"]}],"execution_count":24,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false,"advisor":{"adviceMetadata":"{\"artifactId\":\"85b8f3d7-acef-4c77-9177-790b900f0304\",\"activityId\":\"019b0dda-afc2-4074-9241-4fe0e9137764\",\"applicationId\":\"application_1755784154804_0001\",\"jobGroupId\":\"12\",\"advices\":{\"error\":1}}"}},"id":"c47bc686-3b92-4670-acab-93b972eb2353"},{"cell_type":"code","source":["# Verifica se a tabela Delta já existe\n","tabelas_existentes = [t.name for t in spark.catalog.listTables()]\n","tabela_mov_existe = TABELA_MOVIMENTOS in tabelas_existentes\n","\n","# AJUSTA PROBLEMA DE DATAS < 01/01/1900\n","mov_datajud = mov_datajud.filter(col(\"movimentos_dataHora\") >= \"1900-01-01\") \n","\n","mov_datajud = mov_datajud.withColumn(\"data_ingestao\", lit(datetime.now().isoformat()))\n","\n","# Define política de escrita de datas antigas\n","# spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n","\n","if not tabela_mov_existe:\n","    print(f\"Criando nova tabela Delta: {TABELA_MOVIMENTOS}\")\n","    mov_datajud.write.format(\"delta\").mode(\"overwrite\").saveAsTable(TABELA_MOVIMENTOS)\n","\n","else:\n","    print(f\"Tabela já existe. Iniciando merge com deduplicação por múltiplas colunas\")\n","\n","    # Define a chave de deduplicação\n","    chave_deduplicacao = [\n","        \"_id\",\n","        'numeroProcesso',\n","        \"movimentos_codigo\",\n","        \"movimentos_nome\",\n","        \"movimentos_dataHora\"\n","    ]\n","\n","    # Lê dados existentes apenas com as colunas de deduplicação\n","    df_existente = spark.table(TABELA_MOVIMENTOS).select(*chave_deduplicacao).dropDuplicates()\n","\n","    # Identifica novos registros via anti join\n","    df_novos = mov_datajud.join(df_existente, on=chave_deduplicacao, how=\"left_anti\")\n","\n","    # Verifica se há dados novos\n","    if df_novos.rdd.isEmpty():\n","        print(\"✅ Nenhum dado novo a inserir.\")\n","    else:\n","        total = df_novos.count()\n","        print(f\"Inserindo {total} novos registros...\")\n","        df_novos.write.format(\"delta\").mode(\"append\").saveAsTable(TABELA_MOVIMENTOS)\n","\n","print(\"✅ Ingestão finalizada.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"019b0dda-afc2-4074-9241-4fe0e9137764","normalized_state":"finished","queued_time":"2025-08-21T14:18:47.2432253Z","session_start_time":null,"execution_start_time":"2025-08-21T14:18:47.2443174Z","execution_finish_time":"2025-08-21T14:21:23.485108Z","parent_msg_id":"86c86759-8015-4968-925a-5164d7ec19c8"},"text/plain":"StatementMeta(, 019b0dda-afc2-4074-9241-4fe0e9137764, 11, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Criando nova tabela Delta: movimentos_datajud\n"]},{"output_type":"error","ename":"Py4JJavaError","evalue":"An error occurred while calling o7899.saveAsTable.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 73 in stage 37.0 failed 4 times, most recent failure: Lost task 73.3 in stage 37.0 (TID 7249) (vm-c2983333 executor 19): ExecutorLostFailure (executor 19 exited caused by one of the running tasks) Reason: Container from a bad node: container_1755784154804_0001_01_000020 on host: vm-c2983333. Exit status: 137. Diagnostics: [2025-08-21 14:21:20.846]Container killed on request. Exit code is 137\n[2025-08-21 14:21:20.859]Container exited with a non-zero exit code 137. \n[2025-08-21 14:21:20.861]Killed by external signal\n.\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3055)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2991)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2990)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2990)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1294)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1294)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1294)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3262)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3193)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3182)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1028)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2568)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2589)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2608)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2633)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1056)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:411)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1055)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:320)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:187)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:296)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:179)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:173)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:149)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:145)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$submitShuffleJob$1(ShuffleExchangeExec.scala:73)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:268)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:265)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob(ShuffleExchangeExec.scala:73)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob$(ShuffleExchangeExec.scala:72)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.submitShuffleJob(ShuffleExchangeExec.scala:120)\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture$lzycompute(QueryStageExec.scala:188)\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture(QueryStageExec.scala:188)\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:190)\n\tat org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:61)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$6(AdaptiveSparkPlanExec.scala:297)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$6$adapted(AdaptiveSparkPlanExec.scala:295)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:295)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:961)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:266)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:412)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.finalPhysicalPlan(AdaptiveSparkPlanExec.scala:258)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.materializeAdaptiveSparkPlan$1(DeltaFileFormatWriter.scala:226)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.$anonfun$write$8(DeltaFileFormatWriter.scala:227)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.materializeAdaptiveSparkPlan$1(DeltaFileFormatWriter.scala:227)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.$anonfun$write$8(DeltaFileFormatWriter.scala:227)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.materializeAdaptiveSparkPlan$1(DeltaFileFormatWriter.scala:227)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.$anonfun$write$10(DeltaFileFormatWriter.scala:233)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.write(DeltaFileFormatWriter.scala:233)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.callDeltaFileFormatWriter$1(TransactionalWrite.scala:572)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.$anonfun$tryWriteFiles$5(TransactionalWrite.scala:583)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.$anonfun$tryWriteFiles$3(TransactionalWrite.scala:583)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:132)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:220)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:961)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.tryWriteFiles(TransactionalWrite.scala:519)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:411)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:385)\n\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:150)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:382)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:372)\n\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:150)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:257)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:253)\n\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:150)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:242)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:239)\n\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:150)\n\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.writeFiles(WriteIntoDelta.scala:362)\n\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.writeAndReturnCommitData(WriteIntoDelta.scala:310)\n\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.doDeltaWrite$1(CreateDeltaTableCommand.scala:250)\n\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.handleCreateTableAsSelect(CreateDeltaTableCommand.scala:277)\n\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:150)\n\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:110)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:169)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:167)\n\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:57)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:137)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation(SynapseLoggingShim.scala:111)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation$(SynapseLoggingShim.scala:93)\n\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:57)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:136)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:126)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:116)\n\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:57)\n\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:110)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:184)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:169)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:167)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:65)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.org$apache$spark$sql$delta$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:95)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:545)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:169)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:167)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:65)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:507)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:589)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:582)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:576)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:192)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:225)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:214)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:132)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:220)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:961)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:214)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:202)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:36)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:36)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:36)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:202)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:186)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:180)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:262)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:905)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:664)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:593)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","Cell \u001b[0;32mIn[32], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tabela_mov_existe:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCriando nova tabela Delta: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTABELA_MOVIMENTOS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m     mov_datajud\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msaveAsTable(TABELA_MOVIMENTOS)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTabela já existe. Iniciando merge com deduplicação por múltiplas colunas\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py:1586\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1585\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m-> 1586\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msaveAsTable(name)\n","File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n","File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o7899.saveAsTable.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 73 in stage 37.0 failed 4 times, most recent failure: Lost task 73.3 in stage 37.0 (TID 7249) (vm-c2983333 executor 19): ExecutorLostFailure (executor 19 exited caused by one of the running tasks) Reason: Container from a bad node: container_1755784154804_0001_01_000020 on host: vm-c2983333. Exit status: 137. Diagnostics: [2025-08-21 14:21:20.846]Container killed on request. Exit code is 137\n[2025-08-21 14:21:20.859]Container exited with a non-zero exit code 137. \n[2025-08-21 14:21:20.861]Killed by external signal\n.\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3055)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2991)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2990)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2990)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1294)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1294)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1294)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3262)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3193)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3182)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1028)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2568)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2589)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2608)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2633)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1056)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:411)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1055)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:320)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:187)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:296)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:179)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:173)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:149)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:145)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$submitShuffleJob$1(ShuffleExchangeExec.scala:73)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:268)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:265)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob(ShuffleExchangeExec.scala:73)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob$(ShuffleExchangeExec.scala:72)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.submitShuffleJob(ShuffleExchangeExec.scala:120)\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture$lzycompute(QueryStageExec.scala:188)\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture(QueryStageExec.scala:188)\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:190)\n\tat org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:61)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$6(AdaptiveSparkPlanExec.scala:297)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$6$adapted(AdaptiveSparkPlanExec.scala:295)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:295)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:961)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:266)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:412)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.finalPhysicalPlan(AdaptiveSparkPlanExec.scala:258)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.materializeAdaptiveSparkPlan$1(DeltaFileFormatWriter.scala:226)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.$anonfun$write$8(DeltaFileFormatWriter.scala:227)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.materializeAdaptiveSparkPlan$1(DeltaFileFormatWriter.scala:227)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.$anonfun$write$8(DeltaFileFormatWriter.scala:227)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.materializeAdaptiveSparkPlan$1(DeltaFileFormatWriter.scala:227)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.$anonfun$write$10(DeltaFileFormatWriter.scala:233)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.write(DeltaFileFormatWriter.scala:233)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.callDeltaFileFormatWriter$1(TransactionalWrite.scala:572)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.$anonfun$tryWriteFiles$5(TransactionalWrite.scala:583)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.$anonfun$tryWriteFiles$3(TransactionalWrite.scala:583)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:132)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:220)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:961)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.tryWriteFiles(TransactionalWrite.scala:519)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:411)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:385)\n\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:150)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:382)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:372)\n\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:150)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:257)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:253)\n\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:150)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:242)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:239)\n\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:150)\n\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.writeFiles(WriteIntoDelta.scala:362)\n\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.writeAndReturnCommitData(WriteIntoDelta.scala:310)\n\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.doDeltaWrite$1(CreateDeltaTableCommand.scala:250)\n\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.handleCreateTableAsSelect(CreateDeltaTableCommand.scala:277)\n\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:150)\n\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:110)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:169)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:167)\n\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:57)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:137)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation(SynapseLoggingShim.scala:111)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation$(SynapseLoggingShim.scala:93)\n\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:57)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:136)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:126)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:116)\n\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:57)\n\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:110)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:184)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:169)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:167)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:65)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.org$apache$spark$sql$delta$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:95)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:545)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:169)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:167)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:65)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:507)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:589)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:582)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:576)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:192)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:225)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:214)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:132)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:220)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:961)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:214)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:202)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:36)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:36)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:36)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:202)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:186)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:180)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:262)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:905)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:664)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:593)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"]}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"advisor":{"adviceMetadata":"{\"artifactId\":\"85b8f3d7-acef-4c77-9177-790b900f0304\",\"activityId\":\"019b0dda-afc2-4074-9241-4fe0e9137764\",\"applicationId\":\"application_1755784154804_0001\",\"jobGroupId\":\"11\",\"advices\":{\"error\":1}}"}},"id":"eac4ae62-e1b6-44f8-b054-5aedef1cde15"},{"cell_type":"code","source":["# display(spark.sql(\"\"\"\n","# SELECT *\n","# FROM DOL_arqs_auxiliares.movimentos_datajud\n","# limit 100\"\"\"))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":13,"statement_ids":[13],"state":"finished","livy_statement_state":"available","session_id":"f885abcb-2cc9-4f14-89ef-7565e69ca288","normalized_state":"finished","queued_time":"2025-08-13T21:36:12.5575324Z","session_start_time":null,"execution_start_time":"2025-08-13T21:36:12.5590406Z","execution_finish_time":"2025-08-13T21:36:24.319118Z","parent_msg_id":"2e285193-afe8-4fe3-a8cc-ade216d44256"},"text/plain":"StatementMeta(, f885abcb-2cc9-4f14-89ef-7565e69ca288, 13, Finished, Available, Finished)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"f4ee6c05-4a0b-4e52-b598-a40ed010a59d","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, f4ee6c05-4a0b-4e52-b598-a40ed010a59d)"},"metadata":{}}],"execution_count":11,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"03203c97-7013-42f7-b9a0-d1b715e356ae"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{"f4ee6c05-4a0b-4e52-b598-a40ed010a59d":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"TJSP_G1_16098523320168260224","1":"16098523320168260224","2":"51","3":"Conclusão","4":"2024-08-15 12:45:55","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":0,"key":0},{"0":"TJSP_G1_16098523320168260224","1":"16098523320168260224","2":"51","3":"Conclusão","4":"2024-07-22 18:15:58","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":1,"key":1},{"0":"TJSP_G1_16098523320168260224","1":"16098523320168260224","2":"51","3":"Conclusão","4":"2024-01-26 12:36:53","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":2,"key":2},{"0":"TJSP_G1_16098523320168260224","1":"16098523320168260224","2":"51","3":"Conclusão","4":"2022-07-25 20:01:22","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":3,"key":3},{"0":"TJSP_G1_16098523320168260224","1":"16098523320168260224","2":"51","3":"Conclusão","4":"2022-05-06 11:01:50","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":4,"key":4},{"0":"TJSP_G1_16098523320168260224","1":"16098523320168260224","2":"51","3":"Conclusão","4":"2020-08-13 17:42:21","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":5,"key":5},{"0":"TJSP_G1_16098523320168260224","1":"16098523320168260224","2":"51","3":"Conclusão","4":"2020-07-02 14:04:51","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":6,"key":6},{"0":"TJSP_G1_16098523320168260224","1":"16098523320168260224","2":"51","3":"Conclusão","4":"2017-08-05 11:23:09","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":7,"key":7},{"0":"TJSP_G1_15710704920198260224","1":"15710704920198260224","2":"51","3":"Conclusão","4":"2021-11-17 18:10:52","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":8,"key":8},{"0":"TJSP_G1_15710704920198260224","1":"15710704920198260224","2":"51","3":"Conclusão","4":"2021-08-16 16:06:36","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":9,"key":9},{"0":"TJSP_G1_15712341420198260224","1":"15712341420198260224","2":"51","3":"Conclusão","4":"2022-08-23 11:15:54","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":10,"key":10},{"0":"TJSP_G1_15712341420198260224","1":"15712341420198260224","2":"51","3":"Conclusão","4":"2021-08-16 15:57:46","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":11,"key":11},{"0":"TJSP_G1_15716646320198260224","1":"15716646320198260224","2":"51","3":"Conclusão","4":"2022-08-15 14:13:37","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":12,"key":12},{"0":"TJSP_G1_15716646320198260224","1":"15716646320198260224","2":"51","3":"Conclusão","4":"2021-08-17 17:47:49","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":13,"key":13},{"0":"TJSP_G1_15717356520198260224","1":"15717356520198260224","2":"51","3":"Conclusão","4":"2022-08-15 19:33:35","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":14,"key":14},{"0":"TJSP_G1_15717356520198260224","1":"15717356520198260224","2":"51","3":"Conclusão","4":"2021-08-16 16:09:21","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":15,"key":15},{"0":"TJSP_G1_15720465620198260224","1":"15720465620198260224","2":"51","3":"Conclusão","4":"2022-08-15 14:15:49","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":16,"key":16},{"0":"TJSP_G1_15720465620198260224","1":"15720465620198260224","2":"51","3":"Conclusão","4":"2021-08-16 16:25:04","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":17,"key":17},{"0":"TJSP_G1_15726476220198260224","1":"15726476220198260224","2":"51","3":"Conclusão","4":"2022-12-03 14:12:35","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":18,"key":18},{"0":"TJSP_G1_15726476220198260224","1":"15726476220198260224","2":"51","3":"Conclusão","4":"2021-08-17 15:28:05","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":19,"key":19},{"0":"TJSP_G1_15740150920198260224","1":"15740150920198260224","2":"51","3":"Conclusão","4":"2022-08-16 10:29:29","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":20,"key":20},{"0":"TJSP_G1_15740150920198260224","1":"15740150920198260224","2":"51","3":"Conclusão","4":"2021-08-16 17:53:01","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":21,"key":21},{"0":"TJSP_G1_15747807720198260224","1":"15747807720198260224","2":"51","3":"Conclusão","4":"2022-07-23 23:53:50","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":22,"key":22},{"0":"TJSP_G1_15747807720198260224","1":"15747807720198260224","2":"51","3":"Conclusão","4":"2022-01-24 18:32:03","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":23,"key":23},{"0":"TJSP_G1_15747807720198260224","1":"15747807720198260224","2":"51","3":"Conclusão","4":"2021-08-16 17:55:31","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":24,"key":24},{"0":"TJSP_G1_15750134520178260224","1":"15750134520178260224","2":"51","3":"Conclusão","4":"2022-03-30 13:51:16","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":25,"key":25},{"0":"TJSP_G1_15750134520178260224","1":"15750134520178260224","2":"51","3":"Conclusão","4":"2021-04-16 09:52:18","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":26,"key":26},{"0":"TJSP_G1_15750134520178260224","1":"15750134520178260224","2":"51","3":"Conclusão","4":"2019-03-25 11:39:02","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":27,"key":27},{"0":"TJSP_G1_15754935220198260224","1":"15754935220198260224","2":"51","3":"Conclusão","4":"2022-08-23 11:52:58","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":28,"key":28},{"0":"TJSP_G1_15754935220198260224","1":"15754935220198260224","2":"51","3":"Conclusão","4":"2021-08-16 17:13:38","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":29,"key":29},{"0":"TJSP_G1_15756217220198260224","1":"15756217220198260224","2":"51","3":"Conclusão","4":"2022-08-08 19:35:28","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":30,"key":30},{"0":"TJSP_G1_15756217220198260224","1":"15756217220198260224","2":"51","3":"Conclusão","4":"2021-08-18 15:48:26","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":31,"key":31},{"0":"TJSP_G1_15759550920198260224","1":"15759550920198260224","2":"51","3":"Conclusão","4":"2022-07-19 23:02:43","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":32,"key":32},{"0":"TJSP_G1_15759550920198260224","1":"15759550920198260224","2":"51","3":"Conclusão","4":"2021-04-07 14:04:36","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":33,"key":33},{"0":"TJSP_G1_15759550920198260224","1":"15759550920198260224","2":"51","3":"Conclusão","4":"2019-10-14 11:04:33","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":34,"key":34},{"0":"TJSP_G1_15764797420178260224","1":"15764797420178260224","2":"51","3":"Conclusão","4":"2021-04-15 02:14:08","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":35,"key":35},{"0":"TJSP_G1_15764797420178260224","1":"15764797420178260224","2":"51","3":"Conclusão","4":"2021-04-15 00:40:22","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":36,"key":36},{"0":"TJSP_G1_15764797420178260224","1":"15764797420178260224","2":"51","3":"Conclusão","4":"2019-03-25 13:00:11","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":37,"key":37},{"0":"TJSP_G1_15765325520178260224","1":"15765325520178260224","2":"51","3":"Conclusão","4":"2022-01-21 18:57:24","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":38,"key":38},{"0":"TJSP_G1_15765325520178260224","1":"15765325520178260224","2":"51","3":"Conclusão","4":"2019-03-23 13:25:13","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":39,"key":39},{"0":"TJSP_G1_15768454520198260224","1":"15768454520198260224","2":"51","3":"Conclusão","4":"2022-06-26 02:05:06","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":40,"key":40},{"0":"TJSP_G1_15768454520198260224","1":"15768454520198260224","2":"51","3":"Conclusão","4":"2021-08-17 17:23:37","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":41,"key":41},{"0":"TJSP_G1_15772501820188260224","1":"15772501820188260224","2":"51","3":"Conclusão","4":"2022-07-07 19:24:08","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":42,"key":42},{"0":"TJSP_G1_15772501820188260224","1":"15772501820188260224","2":"51","3":"Conclusão","4":"2021-01-14 14:59:44","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":43,"key":43},{"0":"TJSP_G1_15777565720198260224","1":"15777565720198260224","2":"51","3":"Conclusão","4":"2022-08-23 12:08:14","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":44,"key":44},{"0":"TJSP_G1_15777565720198260224","1":"15777565720198260224","2":"51","3":"Conclusão","4":"2021-08-18 16:44:02","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":45,"key":45},{"0":"TJSP_G1_15777634920198260224","1":"15777634920198260224","2":"51","3":"Conclusão","4":"2021-08-17 13:41:59","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":46,"key":46},{"0":"TJSP_G1_15777660420198260224","1":"15777660420198260224","2":"51","3":"Conclusão","4":"2022-07-01 14:08:48","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":47,"key":47},{"0":"TJSP_G1_15788735420178260224","1":"15788735420178260224","2":"51","3":"Conclusão","4":"2022-07-19 22:44:46","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":48,"key":48},{"0":"TJSP_G1_15788735420178260224","1":"15788735420178260224","2":"51","3":"Conclusão","4":"2021-02-06 14:03:07","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":49,"key":49},{"0":"TJSP_G1_15788735420178260224","1":"15788735420178260224","2":"51","3":"Conclusão","4":"2019-10-25 14:31:46","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":50,"key":50},{"0":"TJSP_G1_15792782220198260224","1":"15792782220198260224","2":"51","3":"Conclusão","4":"2022-05-31 15:55:00","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":51,"key":51},{"0":"TJSP_G1_15792782220198260224","1":"15792782220198260224","2":"51","3":"Conclusão","4":"2020-11-09 13:49:15","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":52,"key":52},{"0":"TJSP_G1_15798028720178260224","1":"15798028720178260224","2":"51","3":"Conclusão","4":"2022-02-18 15:27:53","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":53,"key":53},{"0":"TJSP_G1_15798028720178260224","1":"15798028720178260224","2":"51","3":"Conclusão","4":"2019-03-26 10:44:41","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":54,"key":54},{"0":"TJSP_G1_15807615820178260224","1":"15807615820178260224","2":"51","3":"Conclusão","4":"2022-07-01 09:37:10","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":55,"key":55},{"0":"TJSP_G1_15807615820178260224","1":"15807615820178260224","2":"51","3":"Conclusão","4":"2021-04-20 15:35:02","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":56,"key":56},{"0":"TJSP_G1_15807615820178260224","1":"15807615820178260224","2":"51","3":"Conclusão","4":"2021-04-19 16:35:12","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":57,"key":57},{"0":"TJSP_G1_15807615820178260224","1":"15807615820178260224","2":"51","3":"Conclusão","4":"2019-10-28 11:01:42","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":58,"key":58},{"0":"TJSP_G1_15807670220168260224","1":"15807670220168260224","2":"51","3":"Conclusão","4":"2022-06-01 22:07:02","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":59,"key":59},{"0":"TJSP_G1_15807670220168260224","1":"15807670220168260224","2":"51","3":"Conclusão","4":"2017-05-06 12:38:00","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":60,"key":60},{"0":"TJSP_G1_15815944220188260224","1":"15815944220188260224","2":"51","3":"Conclusão","4":"2022-04-07 00:48:48","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":61,"key":61},{"0":"TJSP_G1_15815944220188260224","1":"15815944220188260224","2":"51","3":"Conclusão","4":"2021-01-12 10:33:42","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":62,"key":62},{"0":"TJSP_G1_15816365720198260224","1":"15816365720198260224","2":"51","3":"Conclusão","4":"2022-05-24 12:20:16","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":63,"key":63},{"0":"TJSP_G1_15816365720198260224","1":"15816365720198260224","2":"51","3":"Conclusão","4":"2020-09-05 12:32:06","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":64,"key":64},{"0":"TJSP_G1_15817448620198260224","1":"15817448620198260224","2":"51","3":"Conclusão","4":"2020-11-06 17:00:01","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":65,"key":65},{"0":"TJSP_G1_15819308020178260224","1":"15819308020178260224","2":"51","3":"Conclusão","4":"2022-06-02 21:08:37","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":66,"key":66},{"0":"TJSP_G1_15819308020178260224","1":"15819308020178260224","2":"51","3":"Conclusão","4":"2019-05-14 22:49:33","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":67,"key":67},{"0":"TJSP_G1_15859199420178260224","1":"15859199420178260224","2":"51","3":"Conclusão","4":"2022-01-20 19:32:03","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":68,"key":68},{"0":"TJSP_G1_15859199420178260224","1":"15859199420178260224","2":"51","3":"Conclusão","4":"2019-05-15 00:12:48","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":69,"key":69},{"0":"TJSP_G1_15864877620188260224","1":"15864877620188260224","2":"51","3":"Conclusão","4":"2022-03-30 13:41:21","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":70,"key":70},{"0":"TJSP_G1_15864877620188260224","1":"15864877620188260224","2":"51","3":"Conclusão","4":"2021-03-31 20:53:24","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":71,"key":71},{"0":"TJSP_G1_15864877620188260224","1":"15864877620188260224","2":"51","3":"Conclusão","4":"2019-10-03 14:28:14","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":72,"key":72},{"0":"TJSP_G1_15867720620178260224","1":"15867720620178260224","2":"51","3":"Conclusão","4":"2021-04-10 02:03:27","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":73,"key":73},{"0":"TJSP_G1_15867720620178260224","1":"15867720620178260224","2":"51","3":"Conclusão","4":"2018-08-23 18:16:46","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":74,"key":74},{"0":"TJSP_G1_15869635120178260224","1":"15869635120178260224","2":"51","3":"Conclusão","4":"2020-06-26 14:08:52","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":75,"key":75},{"0":"TJSP_G1_15869635120178260224","1":"15869635120178260224","2":"51","3":"Conclusão","4":"2020-06-25 19:31:43","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":76,"key":76},{"0":"TJSP_G1_15869635120178260224","1":"15869635120178260224","2":"51","3":"Conclusão","4":"2019-05-15 00:51:08","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":77,"key":77},{"0":"TJSP_G1_15871829320198260224","1":"15871829320198260224","2":"51","3":"Conclusão","4":"2020-09-11 15:37:55","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":78,"key":78},{"0":"TJSP_G1_15883189620178260224","1":"15883189620178260224","2":"51","3":"Conclusão","4":"2022-08-19 14:07:56","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":79,"key":79},{"0":"TJSP_G1_15883189620178260224","1":"15883189620178260224","2":"51","3":"Conclusão","4":"2021-10-23 02:20:27","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":80,"key":80},{"0":"TJSP_G1_15883189620178260224","1":"15883189620178260224","2":"51","3":"Conclusão","4":"2021-06-29 12:19:53","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":81,"key":81},{"0":"TJSP_G1_15883189620178260224","1":"15883189620178260224","2":"51","3":"Conclusão","4":"2021-05-04 20:14:06","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":82,"key":82},{"0":"TJSP_G1_15883189620178260224","1":"15883189620178260224","2":"51","3":"Conclusão","4":"2018-10-13 15:20:29","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":83,"key":83},{"0":"TJSP_G1_15891044320178260224","1":"15891044320178260224","2":"51","3":"Conclusão","4":"2022-01-24 15:28:04","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":84,"key":84},{"0":"TJSP_G1_15891044320178260224","1":"15891044320178260224","2":"51","3":"Conclusão","4":"2019-03-02 15:10:40","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":85,"key":85},{"0":"TJSP_G1_15894032020178260224","1":"15894032020178260224","2":"51","3":"Conclusão","4":"2022-04-04 14:49:00","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":86,"key":86},{"0":"TJSP_G1_15894032020178260224","1":"15894032020178260224","2":"51","3":"Conclusão","4":"2021-04-15 02:34:35","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":87,"key":87},{"0":"TJSP_G1_15894032020178260224","1":"15894032020178260224","2":"51","3":"Conclusão","4":"2021-04-15 00:45:52","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":88,"key":88},{"0":"TJSP_G1_15894032020178260224","1":"15894032020178260224","2":"51","3":"Conclusão","4":"2019-03-28 11:36:38","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":89,"key":89},{"0":"TJSP_G1_15898255820188260224","1":"15898255820188260224","2":"51","3":"Conclusão","4":"2024-10-29 15:25:36","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":90,"key":90},{"0":"TJSP_G1_15898255820188260224","1":"15898255820188260224","2":"51","3":"Conclusão","4":"2024-10-29 10:59:48","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":91,"key":91},{"0":"TJSP_G1_15898255820188260224","1":"15898255820188260224","2":"51","3":"Conclusão","4":"2024-04-10 17:34:43","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":92,"key":92},{"0":"TJSP_G1_15898255820188260224","1":"15898255820188260224","2":"51","3":"Conclusão","4":"2022-07-18 23:03:15","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":93,"key":93},{"0":"TJSP_G1_15917465220188260224","1":"15917465220188260224","2":"51","3":"Conclusão","4":"2022-06-02 20:25:47","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":94,"key":94},{"0":"TJSP_G1_15917465220188260224","1":"15917465220188260224","2":"51","3":"Conclusão","4":"2021-01-18 16:24:35","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":95,"key":95},{"0":"TJSP_G1_15936073420228260224","1":"15936073420228260224","2":"51","3":"Conclusão","4":"2023-04-04 10:39:12","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":96,"key":96},{"0":"TJSP_G1_15952012520188260224","1":"15952012520188260224","2":"51","3":"Conclusão","4":"2022-05-02 14:54:56","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":97,"key":97},{"0":"TJSP_G1_15952012520188260224","1":"15952012520188260224","2":"51","3":"Conclusão","4":"2021-01-22 19:47:38","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":98,"key":98},{"0":"TJSP_G1_15986777120188260224","1":"15986777120188260224","2":"51","3":"Conclusão","4":"2022-04-26 13:13:40","5":"3","6":"tipo_de_conclusao","7":"6","8":"para decisão","9":"2025-07-22 15:29:59","10":"2025-08-13T21:20:19.16965","index":99,"key":99}],"schema":[{"key":"0","name":"_id","type":"string"},{"key":"1","name":"numeroProcesso","type":"string"},{"key":"2","name":"movimentos_codigo","type":"bigint"},{"key":"3","name":"movimentos_nome","type":"string"},{"key":"4","name":"movimentos_dataHora","type":"timestamp"},{"key":"5","name":"movimentos_complementosTabelados_codigo","type":"bigint"},{"key":"6","name":"movimentos_complementosTabelados_descricao","type":"string"},{"key":"7","name":"movimentos_complementosTabelados_valor","type":"bigint"},{"key":"8","name":"movimentos_complementosTabelados_nome","type":"string"},{"key":"9","name":"data_download","type":"string"},{"key":"10","name":"data_ingestao","type":"string"}],"truncated":false},"isSummary":false,"language":"scala","wranglerEntryContext":{"dataframeType":"pyspark"}},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":[],"seriesFieldKeys":[],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"},"viewOptionsGroup":[{"tabItems":[{"type":"table","name":"Table","key":"0","options":{}}]}]}}}}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"722beffc-7d1c-48ad-bb0b-9acd81701245","known_lakehouses":[{"id":"722beffc-7d1c-48ad-bb0b-9acd81701245"}],"default_lakehouse_name":"DOL_arqs_auxiliares","default_lakehouse_workspace_id":"0d7b6642-7157-4d38-8bb5-db1acee1c79d"}}},"nbformat":4,"nbformat_minor":5}